%************************************************
% METHODS
%************************************************

\chapter{Methods} \label{chap: methods}

\paragraph{} \textit{This chapter provides a high-level overview of how the dataset has been processed, of which models have been applied on which tasks and of the reasoning behind the application of certain models to certain tasks.}

%------------------------------------------------
% Data analysis and preprocessing
%------------------------------------------------

\section{Data analysis and preprocessing} \label{sec: data_analysis_preprocessing}
\paragraph{} The data collected for this project consists of \acsp{ieeg} generated from real measurements on a patient suffering from epilepsy. The data, corresponding to 24 hours of monitoring and containing three seizures, have been prepared and provided to us by a research group of trained clinicians and epileptologists, in order to be used for this project.

In order to familiarize with the data, in the first phase of the process we conducted a simple data analysis. We computed some basic statistical metrics on the \acsp{ieeg} and we generated some plots of the data to understand them better. In this way, we identified the number of electrodes used for the measurements, the number of epileptic seizures available in the dataset and their duration and, through the plots, we could also visualize the behaviour of brain's electrical signals during the seizures (see Section \ref{sec: data_analysis}).

After that, the data has been preprocessed depending on the model and the task concerned. First, the dataset has been divided in a training set and a test set: the training set is the data that the model analyzes in order to learn from it and to identify patterns, while the test data is used to verify how much the trained model can generalize its knowledge to data that it never saw before. Since our dataset contained only three seizures, we used two seizures for the training set and the remaining one for the test set. Because of the lack of data, we could not create a validation set, so we were forced to mainly use the test set in order to evaluate the models. In most cases, the training set has been standardized, since some models, including neural networks, are very sensible to the numerical range of data features.

For the detection task on a single time step, the data has been shaped so that each sample consisted of a single time step with an associated target, that needs to be predicted. For both the detection and prediction tasks on a sequence, on the other hands, sequences needed to be generated. Therefore, the dataset has been converted in a set of sequences with a single target associated. Depending on whether we were working on detection or on prediction task, the target of each sequence corresponded to the target of the last time step in the sequence, or to the target of a future time step outside the sequence. For the graph-based models, we transformed the samples from being sequences of time steps to being sequences of graphs by computing the functional connectivity between electrodes (see Section \ref{sec: fc_graphs}). A deeper explanation about how the data has been preprocessed can be found in Section \ref{sec: data_preprocessing}.

%----------------------------------------------------------------
% Application of the models to the prediction tasks
%----------------------------------------------------------------

\section{Application of the models to the prediction tasks} \label{sec: models_application}

\paragraph{} The scope of this project was to conduct a study on different machine learning and deep learning models applied to the problem of seizure prediction, generating a review of models to solve this problem and looking for the best configuration for each of them. The common thread we set throughout all the project was the lack of data to train the models. Epileptic seizure data, indeed, are very difficult to gather, so we wanted to find out how well the models would have performed in a real-life scenario, having a very restricted amount of data available to learn from.

For the choice of the machine learning and deep learning models to include in the review of methods, we wanted both to work with commonly used models and to apply suitable models for the different seizure prediction tasks. For these reasons, the choice fell on the following methods: random forest, gradient boosting and \acs{svm} models representing the group of classic machine learning methods; dense, convolutional and \acs{lstm} neural networks representing the group of standard deep learning models; graph-based convolutional and \acs{lstm} neural networks representing the group of graph-based deep learning models. We applied the chosen models to the preprocessed data in order to solve the three cases of the prediction problem identified in Section \ref{sec: problem_definition}: detection on a time step, detection on a sequence and prediction on a sequence. Each model has been tested on the type of tasks that we though it fitted the most, based on the power and the complexity of the model.

Several experiments have been carried out in order to conduct hyperparameter search and to look for the best configuration of the models for this type of tasks. All the models have been evaluated using loss, accuracy, ROC-AUC and recall, trying to find the best trade-off between the four metrics to chose the models configuration that performed better. The metrics have been computed by taking the mean over 3-fold cross-validation to have more realistic results.

\paragraph{Detection on a time step} The classic machine learning algorithms (random forest, gradient boosting and \acs{svm}) and the dense neural network have been used for the detection task on a single time step.

We tested some models on this type of task for completeness, in order to examine the performance on the baseline case of prediction, but we did not dwell on the search for the best solution as we did not expect to obtain very good results. Given the complex nature of the problem, it is more reasonable to assume that the information determining the presence of an epileptic seizure is contained in a sequence of time steps and in the relation between their corresponding brain activity's values, not in the values of a single time step.

\paragraph{Detection on a sequence} Standard and graph-based convolutional and \acs{lstm} neural networks have been tested on the detection task on a sequence. 

Like in the previous case, for the detection task on a sequence we did not experiment as much as for the prediction task on a sequence, but our intention was to gather some knowledge about the behaviour of the models in this second baseline situation. In this case, we wanted to understand if the added information from time steps prior to the one we wanted to predict could be decisive for the prediction. We supposed that, by having available also the information from previous time steps, the models should have been able to perform better on the task of detection on a sequence with respect to the task of detection on a single time step.

\paragraph{Prediction on a sequence} Standard and graph-based convolutional and \acs{lstm} neural networks have been tested also on the detection task on a sequence.

The prediction task on a sequence has been the main focus of the project, since it is the most useful for real-world applications; therefore, it was also the task for which we spent more time looking for the best solution. In this case, we were trying to understand whether the information of a sequence of time steps some time in the past were crucial to predict a time step in the future with respect to the sequence. The experiments have been conducted using different time distances between the sequence and the corresponding future time step to predict. In this way, we could determine how far the models where able to predict and which was the distance with which they performed the best.

\paragraph{} The choice of the models to test on the different prediction tasks has been based on the models abilities. Random forest, gradient boosting, \acs{svm} and dense neural network models have been used for the detection task on a time step because their level of complexity seems to be suitable for the problem and also because they are not able to process sequences of data, so they could not have been useful for the other two problem's cases. On the other hand, standard and graph-based convolutional and \acs{lstm} neural network models have been used both for the detection and prediction tasks on a sequence, both because of the higher complexity required by the problem and because they are convenient models to process sequences.
